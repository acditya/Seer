# Seer ğŸ”®

**See through sound** - A voice-first navigation assistant for visually impaired users.

## Features

- ğŸ¤ **Voice-first interface** - Press-to-talk interaction
- ğŸ‘ï¸ **GPT-4o-mini vision** - AI sees through your camera in real-time
- ğŸ§  **Intelligent guidance** - Natural language directions to any destination
- ğŸ”Š **Natural speech** - iOS built-in text-to-speech
- ğŸ“ **Checkpoint navigation** - "Take me to the door", "Find the elevator"
- ğŸ“³ **Haptic feedback** - Feel when recording/navigating
- â™¿ **Accessibility-focused** - Large touch targets, voice feedback

## Tech Stack

**Client:** Expo 54 (React Native + TypeScript)  
**Server:** Flask (Python)  
**AI:** OpenAI (Whisper STT + GPT-4o-mini Vision) + iOS TTS

## Quick Start

### 1. Server Setup

```bash
cd server
python -m venv .venv
.venv\Scripts\activate          # Windows
# source .venv/bin/activate     # Mac/Linux

pip install -r requirements.txt
python main.py
```

### 2. Configure Environment

Create `server/.env`:

```bash
PORT=8000

# OpenAI API (for everything: Whisper STT + GPT-4o-mini vision)
OPENAI_API_KEY=sk-proj-your_key_here
```

### 3. Client Setup

```bash
cd client
npm install

# Update app.config.ts with your local IP
# serverUrl: 'http://YOUR_IP:8000'

npm start
```

### 4. Run on iPhone

1. Install Expo Go from App Store
2. Scan QR code from terminal
3. Grant camera & microphone permissions
4. Start navigating!

## Usage

**Simple & Interrupt-based:**

1. **Press and hold** the green button
2. **Say your destination**: "elevator", "exit", "bathroom"
3. **Release** â†’ App captures scene, analyzes, speaks ONCE
4. **Press again** to check scene or give commands
5. **Feel haptic feedback** for key events

**New flow:** No continuous loop! Press button â†’ Analyze â†’ Speak â†’ Wait for next press.

### Voice Commands

- **"repeat"** - Hear last instruction again
- **"stop"** - End navigation
- **"new checkpoint [name]"** - Change destination
- **"I'm here"** - Mark destination reached

## Project Structure

```
seer/
â”œâ”€â”€ client/          # Expo React Native app
â”‚   â”œâ”€â”€ App.tsx
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api.ts                    # Server communication
â”‚   â”‚   â”œâ”€â”€ hooks/useNavigationLoop.ts # Navigation state machine
â”‚   â”‚   â””â”€â”€ components/               # UI components
â”‚   â””â”€â”€ app.config.ts                 # App configuration
â”‚
â””â”€â”€ server/          # Flask Python API
    â”œâ”€â”€ main.py                       # API endpoints
    â”œâ”€â”€ stt.py                        # OpenAI Whisper STT
    â”œâ”€â”€ tts.py                        # Text passthrough (iOS TTS on client)
    â”œâ”€â”€ plan.py                       # GPT-4o-mini vision navigation
    â””â”€â”€ state.py                      # Scene state tracking
```

## API Endpoints

- `POST /stt` - Speech to text (OpenAI Whisper)
- `POST /tts` - Text passthrough (client uses iOS TTS)
- `POST /plan` - Vision-based navigation (GPT-4o-mini analyzes camera image)
- `GET /scene` - Current scene state

## Development

**Server:**
```bash
cd server && python main.py
```

**Client:**
```bash
cd client && npm start
```

## Troubleshooting

**"Network error"**
- Update `serverUrl` in `client/app.config.ts` with your local IP
- Ensure server is running: visit `http://localhost:8000`
- Check both devices on same WiFi

**No camera preview**
- Grant camera permissions in iOS Settings â†’ Expo Go

**Audio doesn't play**
- Check iPhone is not on silent mode
- Grant microphone permissions

## License

MIT

---

Built for accessibility. Powered by AI. ğŸ¯
